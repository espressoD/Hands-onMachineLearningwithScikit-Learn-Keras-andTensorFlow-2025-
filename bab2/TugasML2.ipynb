{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ““ Notebook - Teori & Praktik Bab 2: Proyek Machine Learning End-to-End\n",
    "\n",
    "Selamat datang di notebook panduan untuk Bab 2! Bab ini adalah bab yang paling penting untuk memahami alur kerja sebuah proyek Machine Learning dari awal hingga akhir. Di sini, kita akan mempraktikkan setiap langkah yang dijelaskan di buku menggunakan dataset **California Housing Prices**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langkah 1 & 2: Memahami Masalah dan Mengambil Data\n",
    "\n",
    "**Teori (Penjelasan LLM):**\n",
    "Setiap proyek ML dimulai dengan tujuan bisnis. Untuk proyek ini, tujuannya adalah **membangun model yang dapat memprediksi harga median rumah di sebuah distrik di California** berdasarkan data sensus.\n",
    "\n",
    "Ini adalah masalah:\n",
    "- **Supervised Learning**: Karena data kita memiliki label (harga median rumah).\n",
    "- **Regresi**: Karena kita memprediksi sebuah nilai numerik.\n",
    "- **Batch Learning**: Karena kita akan melatih model menggunakan semua data yang ada sekaligus.\n",
    "\n",
    "Kita akan mulai dengan mengambil data dan melihat strukturnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup awal: Impor library yang dibutuhkan\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "\n",
    "# Lokasi download data\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    \"\"\"Fungsi untuk mengunduh dan mengekstrak data.\"\"\"\n",
    "    os.makedirs(housing_path, exist_ok=True)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    \"\"\"Fungsi untuk memuat data dari file CSV.\"\"\"\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "# Jalankan fungsi untuk mendapatkan data\n",
    "fetch_housing_data()\n",
    "housing = load_housing_data()\n",
    "\n",
    "# Lihat 5 baris pertama dari data\n",
    "print(\"Data 5 baris pertama:\")\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lihat informasi umum tentang data\n",
    "print(\"\\nInformasi Dataset:\")\n",
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langkah Penting: Membuat Test Set\n",
    "\n",
    "**Teori (Penjelasan LLM):**\n",
    "Sebelum kita melakukan eksplorasi data lebih jauh, kita **harus** memisahkan *test set*. Mengapa? Untuk menghindari **data snooping bias**.\n",
    "\n",
    "Otak kita sangat pandai menemukan pola. Jika kita melihat *test set* terlalu awal, kita mungkin secara tidak sadar menemukan pola di sana dan memilih model yang bekerja baik untuk *test set* tersebut. Akibatnya, saat kita mengevaluasi model di akhir, hasilnya akan terlalu optimis dan tidak mencerminkan performa model di dunia nyata.\n",
    "\n",
    "Kita akan menggunakan **Stratified Sampling** untuk memastikan *test set* kita representatif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Pertama, buat kategori pendapatan untuk stratified sampling\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"], \n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf], \n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "# Lakukan stratified split berdasarkan kategori pendapatan\n",
    "strat_train_set, strat_test_set = train_test_split(\n",
    "    housing, test_size=0.2, random_state=42, stratify=housing[\"income_cat\"])\n",
    "\n",
    "# Hapus kolom 'income_cat' agar data kembali seperti semula\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)\n",
    "\n",
    "print(f\"Ukuran Training Set: {len(strat_train_set)}\")\n",
    "print(f\"Ukuran Test Set: {len(strat_test_set)}\")\n",
    "\n",
    "# Mulai sekarang, kita hanya akan bekerja dengan 'strat_train_set'.\n",
    "# Kita akan \"melupakan\" 'strat_test_set' sampai akhir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langkah 3: Menjelajahi dan Memvisualisasikan Data\n",
    "\n",
    "**Teori (Penjelasan LLM):**\n",
    "Tujuan dari fase ini adalah untuk mendapatkan *insight* atau pemahaman mendalam tentang data. Dengan visualisasi, kita bisa menemukan korelasi, pola, dan anomali yang mungkin tidak terlihat dari sekadar melihat angka.\n",
    "\n",
    "Kita akan membuat beberapa plot untuk memahami distribusi geografis dan hubungan antar fitur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat salinan training set agar data asli tidak termodifikasi\n",
    "housing = strat_train_set.copy()\n",
    "\n",
    "# Visualisasi geografis\n",
    "# Parameter 'alpha=0.1' membantu melihat area padat penduduk\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1, figsize=(10,7))\n",
    "plt.title(\"Distribusi Geografis Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisasi harga rumah (warna) dan populasi (ukuran)\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
    "             s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n",
    "             c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True)\n",
    "plt.legend()\n",
    "plt.title(\"Harga Rumah vs Lokasi & Populasi\")\n",
    "plt.show()\n",
    "\n",
    "# Insight: Harga rumah sangat berkaitan dengan lokasi (misalnya, dekat laut) dan kepadatan penduduk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mencari Korelasi\n",
    "\n",
    "**Teori (Penjelasan LLM):**\n",
    "Kita bisa menghitung koefisien korelasi standar (Pearson's r) antara setiap pasang atribut. Korelasi berkisar dari -1 (korelasi negatif kuat) hingga 1 (korelasi positif kuat). Nilai mendekati 0 berarti tidak ada korelasi linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()\n",
    "\n",
    "# Lihat korelasi setiap fitur dengan harga median rumah\n",
    "print(corr_matrix[\"median_house_value\"].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eksperimen dengan Kombinasi Atribut\n",
    "\n",
    "**Teori (Penjelasan LLM):**\n",
    "Terkadang, kombinasi beberapa fitur bisa memberikan informasi yang lebih baik daripada fitur-fitur itu sendiri. Ini adalah bagian dari *feature engineering*. Misalnya, jumlah kamar per rumah mungkin lebih informatif daripada total kamar di satu distrik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"] / housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"] / housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"] = housing[\"population\"] / housing[\"households\"]\n",
    "\n",
    "corr_matrix = housing.corr()\n",
    "print(corr_matrix[\"median_house_value\"].sort_values(ascending=False))\n",
    "\n",
    "# Insight: Fitur baru 'rooms_per_household' dan 'bedrooms_per_room' ternyata memiliki korelasi yang lebih baik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langkah 4: Mempersiapkan Data untuk Algoritma ML\n",
    "\n",
    "**Teori (Penjelasan LLM):**\n",
    "Kebanyakan algoritma ML tidak bisa bekerja dengan data yang hilang atau fitur non-numerik. Kita juga perlu memastikan semua fitur memiliki skala yang sama. Proses ini harus diotomatisasi menggunakan fungsi atau *pipeline* agar mudah diterapkan pada data baru.\n",
    "\n",
    "Kita akan melakukan:\n",
    "1. **Data Cleaning**: Mengisi nilai yang hilang.\n",
    "2. **Handling Text/Categorical**: Mengubah fitur teks menjadi angka.\n",
    "3. **Feature Scaling**: Menyamakan skala semua fitur numerik.\n",
    "4. **Transformation Pipelines**: Menggabungkan semua langkah menjadi satu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pisahkan kembali fitur (predictors) dan label (target)\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "\n",
    "# Pisahkan fitur numerik dan kategorikal\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "housing_cat = housing[[\"ocean_proximity\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Cleaning: Mengisi nilai null dengan median\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "imputer.fit(housing_num) # imputer belajar median dari data numerik\n",
    "X = imputer.transform(housing_num)\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index)\n",
    "\n",
    "# 2. Handling Categorical: Menggunakan OneHotEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "print(\"Fitur kategorikal setelah di-encode (sparse matrix):\")\n",
    "print(housing_cat_1hot.toarray()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation Pipelines\n",
    "\n",
    "**Teori (Penjelasan LLM):**\n",
    "Scikit-Learn menyediakan `Pipeline` untuk merangkai beberapa langkah transformasi secara berurutan. Ini sangat berguna untuk menjaga kode tetap rapi dan mengurangi risiko kesalahan. `ColumnTransformer` memungkinkan kita menerapkan pipeline yang berbeda untuk kolom yang berbeda (misalnya, numerik dan kategorikal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Pipeline untuk fitur numerik\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "# ColumnTransformer untuk menggabungkan semua transformasi\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "])\n",
    "\n",
    "# Jalankan pipeline pada data housing\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "\n",
    "print(\"Shape data setelah diproses:\", housing_prepared.shape)\n",
    "print(\"Data siap digunakan untuk melatih model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langkah 5: Memilih dan Melatih Model\n",
    "\n",
    "**Teori (Penjelasan LLM):**\n",
    "Sekarang data kita sudah bersih dan siap pakai. Saatnya melatih beberapa model. Sebaiknya kita mulai dari model yang sederhana, seperti Regresi Linier, lalu coba model yang lebih kompleks.\n",
    "\n",
    "Kita akan mengukur performa menggunakan **Root Mean Square Error (RMSE)**. Semakin kecil RMSE, semakin baik modelnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "# Evaluasi pada training set\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "print(f\"RMSE Regresi Linier pada Training Set: {lin_rmse:.2f}\")\n",
    "\n",
    "# Insight: Error sekitar $68,628. Ini cukup besar, artinya model ini underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "print(f\"RMSE Decision Tree pada Training Set: {tree_rmse:.2f}\")\n",
    "\n",
    "# Insight: Error 0.0! Ini adalah tanda jelas bahwa model ini overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluasi yang Lebih Baik Menggunakan Cross-Validation\n",
    "\n",
    "**Teori (Penjelasan LLM):**\n",
    "Karena model *Decision Tree* overfitting, kita butuh cara evaluasi yang lebih baik. **K-fold Cross-Validation** adalah solusinya. Ia membagi training set menjadi beberapa bagian (misalnya 10), lalu melatih dan mengevaluasi model 10 kali, setiap kali menggunakan bagian yang berbeda sebagai set validasi. Ini memberikan estimasi performa yang lebih stabil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Scikit-learn cross-validation mengharapkan fungsi utility (semakin besar semakin baik),\n",
    "# bukan fungsi loss (semakin kecil semakin baik), jadi kita gunakan 'neg_mean_squared_error'\n",
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels, \n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "\n",
    "display_scores(tree_rmse_scores)\n",
    "\n",
    "# Insight: Rata-rata errornya sekitar $71,200. Ini jauh lebih buruk dari Regresi Linier. Decision Tree jelas overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langkah 6: Fine-Tune Model\n",
    "\n",
    "**Teori (Penjelasan LLM):**\n",
    "Setelah memilih beberapa model yang menjanjikan (misalnya, `RandomForestRegressor`), kita perlu mencari kombinasi *hyperparameter* terbaik. Mencoba satu per satu secara manual sangat melelahkan. **Grid Search** adalah teknik untuk mengotomatiskan proses ini. Ia akan mencoba semua kombinasi hyperparameter yang kita berikan dan menemukan yang terbaik melalui cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Tentukan kombinasi hyperparameter yang ingin dicoba\n",
    "# Peringatan: Proses ini bisa memakan waktu cukup lama!\n",
    "# Kita gunakan sedikit kombinasi agar cepat.\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "]\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "\n",
    "grid_search.fit(housing_prepared, housing_labels)\n",
    "\n",
    "print(\"Hyperparameter terbaik:\", grid_search.best_params_)\n",
    "\n",
    "# Lihat skor terbaik\n",
    "best_rmse = np.sqrt(-grid_search.best_score_)\n",
    "print(f\"RMSE terbaik dari Grid Search: {best_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langkah 7: Evaluasi Final pada Test Set\n",
    "\n",
    "**Teori (Penjelasan LLM):**\n",
    "Setelah menemukan model terbaik dan hyperparameter-nya, inilah saatnya kita melihat performa finalnya pada *test set* yang selama ini kita simpan. Hasil ini akan menjadi estimasi terbaik kita tentang bagaimana model akan bekerja pada data baru di dunia nyata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "# Pisahkan fitur dan label dari test set\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "# Jalankan test set melalui pipeline (HANYA transform, JANGAN fit!)\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "\n",
    "# Buat prediksi final\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "# Hitung RMSE final\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "\n",
    "print(f\"RMSE Final pada Test Set: {final_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kesimpulan\n",
    "\n",
    "Selamat! Anda telah melalui seluruh alur kerja proyek Machine Learning:\n",
    "1. Memahami masalah\n",
    "2. Mengambil data\n",
    "3. Membuat *test set* yang representatif\n",
    "4. Mengeksplorasi data untuk mencari *insight*\n",
    "5. Mempersiapkan data secara otomatis dengan *pipeline*\n",
    "6. Melatih beberapa model dan memilih yang terbaik\n",
    "7. Melakukan *fine-tuning* hyperparameter\n",
    "8. Mengevaluasi model final pada *test set*\n",
    "\n",
    "Langkah-langkah ini adalah fondasi yang akan Anda gunakan berulang kali dalam karier Machine Learning Anda."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}